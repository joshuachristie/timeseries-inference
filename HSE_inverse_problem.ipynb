{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d71b22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0-dev20210603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:43: UserWarning: You are currently using a nightly version of TensorFlow (2.6.0-dev20210603). \n",
      "TensorFlow Addons offers no support for the nightly versions of TensorFlow. Some things might work, some other might not. \n",
      "If you encounter a bug, do not file an issue on GitHub.\n",
      "  UserWarning,\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "print(tf.version.VERSION)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5ee4fd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_filepath_dataset = tf.data.TFRecordDataset.list_files(\"data/train/*.tfrecord\", shuffle=True)\n",
    "train_num_files = train_filepath_dataset.cardinality()\n",
    "train_dataset = train_filepath_dataset.interleave(\n",
    "    lambda x: tf.data.TFRecordDataset(x),\n",
    "    cycle_length=train_num_files,\n",
    "    block_length=1,\n",
    "    num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",
    "    deterministic=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_filepath_dataset = tf.data.TFRecordDataset.list_files(\"data/valid/*.tfrecord\", shuffle=True)\n",
    "validate_num_files = validate_filepath_dataset.cardinality()\n",
    "validate_dataset = validate_filepath_dataset.interleave(\n",
    "    lambda x: tf.data.TFRecordDataset(x),\n",
    "    cycle_length=validate_num_files,\n",
    "    block_length=1,\n",
    "    num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",
    "    deterministic=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "742cb11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_features = {\"population_size\" : tf.io.FixedLenFeature((), tf.int64),\n",
    "                   \"selection_coefficient\" : tf.io.FixedLenFeature((), tf.float32)}\n",
    "sequence_features = {\"raw_trait_frequencies\" : tf.io.RaggedFeature(tf.float32)}\n",
    "\n",
    "pop_size_min = 50\n",
    "pop_size_max = 500\n",
    "sc_min = 0.0\n",
    "sc_max = 1.0\n",
    "# the minimum number of gens actually survived by the trait is min_survival - 1 \n",
    "# (-2 because the starting and final freq are always recorded; +1 because I filter using > not >=)\n",
    "min_survival = 0\n",
    "max_time_steps = 60 # truncate trait frequencies at max_time_steps to avoid OOM issues\n",
    "num_trait_data = 1000\n",
    "\n",
    "def preprocess_and_split_into_tuples(tfrecord):\n",
    "    # parse sequence example and normalise\n",
    "    example = tf.io.parse_sequence_example(tfrecord, context_features=context_features, sequence_features=sequence_features)\n",
    "    trait_frequencies = example[1][\"raw_trait_frequencies\"]\n",
    "    pop_size_norm = (tf.cast(example[0][\"population_size\"], dtype=tf.float32) - pop_size_min) / (pop_size_max - pop_size_min)\n",
    "    sc_norm = (tf.cast(example[0][\"selection_coefficient\"], dtype=tf.float32) - sc_min) / (sc_max - sc_min)\n",
    "    # filter dataset to only include num_trait_data trials in which the trait survives at least min_survival\n",
    "    trait_frequencies = tf.gather(trait_frequencies, tf.where(trait_frequencies.row_lengths() > min_survival), axis=0)\n",
    "        \n",
    "    trait_frequencies = tf.gather(trait_frequencies, tf.random.uniform(shape=[num_trait_data], minval=0,\n",
    "                                                                      maxval=trait_frequencies.nrows(),\n",
    "                                                                      dtype=tf.int32))    \n",
    "    trait_frequencies = trait_frequencies[:, :, 0:max_time_steps] \n",
    "    # cast labels into a list of tensors matching the features\n",
    "    label_tensors = tf.reshape(tf.stack((tf.repeat(pop_size_norm, num_trait_data), tf.repeat(sc_norm, num_trait_data)), axis=1), \n",
    "                               shape=(num_trait_data, 2))   \n",
    "    # convert trait_frequencies to dense tensor (with mask value of -1)\n",
    "    trait_frequencies = tf.squeeze(trait_frequencies, axis=1)\n",
    "    trait_frequencies = trait_frequencies.to_tensor(default_value = -1., \n",
    "                                                    shape=(num_trait_data, max_time_steps))    \n",
    "    # add final dimension (keras requires an input shape of (batch_size, timesteps, features))\n",
    "    trait_frequencies = tf.expand_dims(trait_frequencies, axis=2)\n",
    "\n",
    "    return tf.data.Dataset.zip((tf.data.Dataset.from_tensor_slices(trait_frequencies), tf.data.Dataset.from_tensor_slices(label_tensors)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac6cc8ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py:5065: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_dataset.interleave(\n",
    "    preprocess_and_split_into_tuples,\n",
    "    cycle_length=train_num_files,\n",
    "    block_length=1,\n",
    "    num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",
    "    deterministic=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_dataset = validate_dataset.interleave(\n",
    "    preprocess_and_split_into_tuples,\n",
    "    cycle_length=validate_num_files,\n",
    "    block_length=1,\n",
    "    num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",
    "    deterministic=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a11f8cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "train_dataset = train_dataset.shuffle(buffer_size=num_trait_data*train_num_files, reshuffle_each_iteration=True).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "validate_dataset = validate_dataset.shuffle(buffer_size=num_trait_data*validate_num_files, reshuffle_each_iteration=True).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "masking (Masking)            (None, None, 1)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, None, 150)         48900     \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 150)               138300    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 30)                4530      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 62        \n",
      "=================================================================\n",
      "Total params: 191,792\n",
      "Trainable params: 191,792\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Masking(mask_value = -1., input_shape=(None,1)))\n",
    "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.RNN(tfa.rnn.LayerNormLSTMCell(75, recurrent_dropout=0.25), return_sequences=True)))\n",
    "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.RNN(tfa.rnn.LayerNormLSTMCell(75, recurrent_dropout=0.25), return_sequences=False)))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "model.add(tf.keras.layers.Dense(30, activation='elu', kernel_initializer='he_normal'))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "model.add(tf.keras.layers.Dense(2))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    if epoch < 4:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.1)\n",
    "\n",
    "scheduler_cb = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "earlystopping_cb = tf.keras.callbacks.EarlyStopping(patience=6, restore_best_weights=True)\n",
    "\n",
    "checkpoints = \"checkpoints/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "checkpoint_filepath = checkpoints + \"checkpoint_layernorm_{epoch:02d}\"\n",
    "\n",
    "model_checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=False,\n",
    "    save_freq='epoch')\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(0.01)\n",
    "model.compile(loss=\"mse\", optimizer=optimizer, metrics=[tf.keras.metrics.MeanAbsoluteError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "586/586 [==============================] - 611s 956ms/step - loss: 0.0645 - mean_absolute_error: 0.1536 - val_loss: 0.0203 - val_mean_absolute_error: 0.0828\n",
      "Epoch 2/50\n",
      "586/586 [==============================] - 581s 918ms/step - loss: 0.0217 - mean_absolute_error: 0.0975 - val_loss: 0.0198 - val_mean_absolute_error: 0.0760\n",
      "Epoch 3/50\n",
      "586/586 [==============================] - 564s 893ms/step - loss: 0.0203 - mean_absolute_error: 0.0911 - val_loss: 0.0194 - val_mean_absolute_error: 0.0764\n",
      "Epoch 4/50\n",
      "586/586 [==============================] - 565s 894ms/step - loss: 0.0198 - mean_absolute_error: 0.0889 - val_loss: 0.0197 - val_mean_absolute_error: 0.0762\n",
      "Epoch 5/50\n",
      "586/586 [==============================] - 576s 913ms/step - loss: 0.0196 - mean_absolute_error: 0.0876 - val_loss: 0.0196 - val_mean_absolute_error: 0.0776\n",
      "Epoch 6/50\n",
      "586/586 [==============================] - 569s 897ms/step - loss: 0.0194 - mean_absolute_error: 0.0868 - val_loss: 0.0197 - val_mean_absolute_error: 0.0764\n",
      "Epoch 7/50\n",
      "586/586 [==============================] - 577s 915ms/step - loss: 0.0193 - mean_absolute_error: 0.0863 - val_loss: 0.0190 - val_mean_absolute_error: 0.0767\n",
      "Epoch 8/50\n",
      "586/586 [==============================] - 572s 901ms/step - loss: 0.0192 - mean_absolute_error: 0.0859 - val_loss: 0.0193 - val_mean_absolute_error: 0.0732\n",
      "Epoch 9/50\n",
      "586/586 [==============================] - 566s 896ms/step - loss: 0.0192 - mean_absolute_error: 0.0858 - val_loss: 0.0192 - val_mean_absolute_error: 0.0757\n",
      "Epoch 10/50\n",
      "586/586 [==============================] - 566s 895ms/step - loss: 0.0191 - mean_absolute_error: 0.0852 - val_loss: 0.0198 - val_mean_absolute_error: 0.0779\n",
      "Epoch 11/50\n",
      "586/586 [==============================] - 566s 896ms/step - loss: 0.0191 - mean_absolute_error: 0.0851 - val_loss: 0.0196 - val_mean_absolute_error: 0.0802\n",
      "Epoch 12/50\n",
      "586/586 [==============================] - 567s 897ms/step - loss: 0.0190 - mean_absolute_error: 0.0849 - val_loss: 0.0196 - val_mean_absolute_error: 0.0781\n",
      "Epoch 13/50\n",
      "586/586 [==============================] - 566s 896ms/step - loss: 0.0190 - mean_absolute_error: 0.0846 - val_loss: 0.0198 - val_mean_absolute_error: 0.0786\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_dataset, epochs=50, validation_data=validate_dataset, \n",
    "                    callbacks=[model_checkpoint_cb, scheduler_cb, earlystopping_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f7400086438>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the model weights from epoch 8\n",
    "model.load_weights('best_model_june11')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup test dataset\n",
    "batch_size = 1024\n",
    "test_filepath_dataset = tf.data.TFRecordDataset.list_files(\"data/test/*.tfrecord\", shuffle=True)\n",
    "\n",
    "test_num_files = test_filepath_dataset.cardinality()\n",
    "\n",
    "test_dataset = test_filepath_dataset.interleave(\n",
    "    lambda x: tf.data.TFRecordDataset(x),\n",
    "    cycle_length=test_num_files,\n",
    "    block_length=1,\n",
    "    num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",
    "    deterministic=False)\n",
    "\n",
    "def test_preprocess_and_split_into_tuples(tfrecord):\n",
    "    # parse sequence example and normalise\n",
    "    example = tf.io.parse_sequence_example(tfrecord, context_features=context_features, sequence_features=sequence_features)\n",
    "    trait_frequencies = example[1][\"raw_trait_frequencies\"]\n",
    "    pop_size_norm = (tf.cast(example[0][\"population_size\"], dtype=tf.float32) - pop_size_min) / (pop_size_max - pop_size_min)\n",
    "    sc_norm = (tf.cast(example[0][\"selection_coefficient\"], dtype=tf.float32) - sc_min) / (sc_max - sc_min)\n",
    "    # filter dataset to only include trials in which the trait survives at least min_survival\n",
    "    trait_frequencies = tf.gather(trait_frequencies, tf.where(trait_frequencies.row_lengths() > min_survival), axis=0)\n",
    "    # note that, unlike in the train/valid cases where we randomly sample num_trait_data trajectories,\n",
    "    # we analyse all trajectories (> min_survival) for the test set        \n",
    "    trait_frequencies = trait_frequencies[:, :, 0:max_time_steps] \n",
    "    trait_frequencies = tf.squeeze(trait_frequencies, axis=1)\n",
    "    n_trajectories = trait_frequencies.nrows()\n",
    "    # cast labels into a list of tensors matching the features\n",
    "    label_tensors = tf.reshape(tf.stack((tf.repeat(pop_size_norm, n_trajectories), \n",
    "                                         tf.repeat(sc_norm, n_trajectories)), axis=1), shape=(n_trajectories, 2))   \n",
    "    # convert trait_frequencies to dense tensor (with mask value of -1)\n",
    "    trait_frequencies = trait_frequencies.to_tensor(default_value = -1., \n",
    "                                                    shape=(n_trajectories, max_time_steps))\n",
    "    \n",
    "    # add final dimension (keras requires an input shape of (batch_size, timesteps, features))\n",
    "    trait_frequencies = tf.expand_dims(trait_frequencies, axis=2)\n",
    "\n",
    "    return tf.data.Dataset.zip((tf.data.Dataset.from_tensor_slices(trait_frequencies), tf.data.Dataset.from_tensor_slices(label_tensors)))\n",
    "\n",
    "test_dataset = test_dataset.interleave(\n",
    "    test_preprocess_and_split_into_tuples,\n",
    "    cycle_length=test_num_files,\n",
    "    block_length=1,\n",
    "    num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",
    "    deterministic=False)\n",
    "\n",
    "test_dataset = test_dataset.shuffle(10000).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 149s 246ms/step - loss: 0.0183 - mean_absolute_error: 0.0719\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.01830650120973587, 0.07190001755952835]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate without conditioning on length of trajectories. This includes many trajectories that only last for a \n",
    "# couple of generations or less---these contain little useful statistical information for differentiating between \n",
    "# different parameter values of the model, and as such, they are a major contributor to the MAE) \n",
    "model.evaluate(test_dataset.take(500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 145s 249ms/step - loss: 0.0042 - mean_absolute_error: 0.0366\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.004214114975184202, 0.03659392520785332]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate test set while conditioning on traits to have survived at least 5 generations.\n",
    "# by excluding extremely short trajectories with little statistical information, MAE is roughly halved\n",
    "\n",
    "# not only are short trajectories devoid of useful statistical information, they are also uninteresting from\n",
    "# a (hypothetical) investigator's viewpoint (e.g. it is less interesting to ask \"what is the biological function\n",
    "# of this trait that went extinct in 3 generations?\" than to ask \"what is the biological function of this trait\n",
    "# that became fixed in the population after 57 generations?\")\n",
    "model.evaluate(test_dataset.take(500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 143s 247ms/step - loss: 0.0029 - mean_absolute_error: 0.0322\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0029298816807568073, 0.03219173103570938]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# conditioning on traits that have survived even longer (at least 10 generations) further reduces MAE\n",
    "model.evaluate(test_dataset.take(500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 136s 248ms/step - loss: 0.0434 - mean_absolute_error: 0.1274\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.04337737709283829, 0.12743809819221497]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The test below illustrates the difficulty of inferring parameter values of the generative evolutionary model, \n",
    "# conditional on the stochastic trajectories being short (i.e. having gone extinct in a few generations).\n",
    "# When filtering the test set for trajectories having survived less than 5 generations, MAE is much higher\n",
    "model.evaluate(test_dataset.take(500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
