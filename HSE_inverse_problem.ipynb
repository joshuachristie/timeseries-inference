{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d71b22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0-dev20210603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:43: UserWarning: You are currently using a nightly version of TensorFlow (2.6.0-dev20210603). \n",
      "TensorFlow Addons offers no support for the nightly versions of TensorFlow. Some things might work, some other might not. \n",
      "If you encounter a bug, do not file an issue on GitHub.\n",
      "  UserWarning,\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "print(tf.version.VERSION)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5ee4fd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_filepath_dataset = tf.data.TFRecordDataset.list_files(\"data/train/*.tfrecord\", shuffle=True)\n",
    "train_num_files = train_filepath_dataset.cardinality()\n",
    "train_dataset = train_filepath_dataset.interleave(\n",
    "    lambda x: tf.data.TFRecordDataset(x),\n",
    "    cycle_length=train_num_files,\n",
    "    block_length=1,\n",
    "    num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",
    "    deterministic=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_filepath_dataset = tf.data.TFRecordDataset.list_files(\"data/valid/*.tfrecord\", shuffle=True)\n",
    "validate_num_files = validate_filepath_dataset.cardinality()\n",
    "validate_dataset = validate_filepath_dataset.interleave(\n",
    "    lambda x: tf.data.TFRecordDataset(x),\n",
    "    cycle_length=validate_num_files,\n",
    "    block_length=1,\n",
    "    num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",
    "    deterministic=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "742cb11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_features = {\"population_size\" : tf.io.FixedLenFeature((), tf.int64),\n",
    "                   \"selection_coefficient\" : tf.io.FixedLenFeature((), tf.float32)}\n",
    "sequence_features = {\"raw_trait_frequencies\" : tf.io.RaggedFeature(tf.float32)}\n",
    "\n",
    "pop_size_min = 50\n",
    "pop_size_max = 500\n",
    "sc_min = 0.0\n",
    "sc_max = 1.0\n",
    "min_survival = 0 # number of gens survived is min_survival - 2 (the starting and final freq are always recorded)\n",
    "max_time_steps = 45 # truncate trait frequencies at max_time_steps to avoid OOM issues\n",
    "num_trait_data = 1000\n",
    "\n",
    "def preprocess_and_split_into_tuples(tfrecord):\n",
    "    # parse sequence example and normalise\n",
    "    example = tf.io.parse_sequence_example(tfrecord, context_features=context_features, sequence_features=sequence_features)\n",
    "    trait_frequencies = example[1][\"raw_trait_frequencies\"]\n",
    "    pop_size_norm = (tf.cast(example[0][\"population_size\"], dtype=tf.float32) - pop_size_min) / (pop_size_max - pop_size_min)\n",
    "    sc_norm = (tf.cast(example[0][\"selection_coefficient\"], dtype=tf.float32) - sc_min) / (sc_max - sc_min)\n",
    "    # filter dataset to only include num_trait_data trials in which the trait survives at least min_survival\n",
    "    trait_frequencies = tf.gather(trait_frequencies, tf.where(trait_frequencies.row_lengths() > min_survival), axis=0)\n",
    "        \n",
    "    trait_frequencies = tf.gather(trait_frequencies, tf.random.uniform(shape=[num_trait_data], minval=0,\n",
    "                                                                      maxval=trait_frequencies.nrows(),\n",
    "                                                                      dtype=tf.int32))    \n",
    "    trait_frequencies = trait_frequencies[:, :, 0:max_time_steps] \n",
    "    # cast labels into a list of tensors matching the features\n",
    "    label_tensors = tf.reshape(tf.stack((tf.repeat(pop_size_norm, num_trait_data), tf.repeat(sc_norm, num_trait_data)), axis=1), \n",
    "                               shape=(num_trait_data, 2))   \n",
    "    # convert trait_frequencies to dense tensor (with mask value of -1)\n",
    "    trait_frequencies = tf.squeeze(trait_frequencies, axis=1)\n",
    "    trait_frequencies = trait_frequencies.to_tensor(default_value = -1., \n",
    "                                                    shape=(num_trait_data, max_time_steps))    \n",
    "    # add final dimension (keras requires an input shape of (batch_size, timesteps, features))\n",
    "    trait_frequencies = tf.expand_dims(trait_frequencies, axis=2)\n",
    "\n",
    "    return tf.data.Dataset.zip((tf.data.Dataset.from_tensor_slices(trait_frequencies), tf.data.Dataset.from_tensor_slices(label_tensors)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac6cc8ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py:5065: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_dataset.interleave(\n",
    "    preprocess_and_split_into_tuples,\n",
    "    cycle_length=train_num_files,\n",
    "    block_length=1,\n",
    "    num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",
    "    deterministic=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_dataset = validate_dataset.interleave(\n",
    "    preprocess_and_split_into_tuples,\n",
    "    cycle_length=validate_num_files,\n",
    "    block_length=1,\n",
    "    num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",
    "    deterministic=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a11f8cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "train_dataset = train_dataset.shuffle(buffer_size=num_trait_data*train_num_files, reshuffle_each_iteration=True).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "validate_dataset = validate_dataset.shuffle(buffer_size=num_trait_data*validate_num_files, reshuffle_each_iteration=True).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Masking(mask_value = -1., input_shape=(None,1)))\n",
    "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50, return_sequences=True)))\n",
    "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50, return_sequences=False)))\n",
    "model.add(tf.keras.layers.Dropout(0.3))\n",
    "model.add(tf.keras.layers.Dense(30, activation='elu', kernel_initializer='he_normal'))\n",
    "model.add(tf.keras.layers.Dropout(0.3))\n",
    "model.add(tf.keras.layers.Dense(2))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    if epoch < 3:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.2)\n",
    "\n",
    "scheduler_cb = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "earlystopping_cb = tf.keras.callbacks.EarlyStopping(patience=7, restore_best_weights=True)\n",
    "\n",
    "checkpoints = \"checkpoints/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "checkpoint_filepath = checkpoints + \"checkpoint_vanilla_{epoch:02d}\"\n",
    "\n",
    "model_checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=False,\n",
    "    save_freq='epoch')\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(0.01)\n",
    "model.compile(loss=\"mse\", optimizer=optimizer, metrics=[tf.keras.metrics.MeanAbsoluteError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0788a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_dataset, epochs=50, validation_data=validate_dataset, \n",
    "                    callbacks=[model_checkpoint_cb, scheduler_cb, earlystopping_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f322017d588>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('checkpoints/20210610-192445checkpoint_layernorm_09')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "masking (Masking)            (None, None, 1)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, None, 120)         31920     \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 120)               89040     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 30)                3630      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 62        \n",
      "=================================================================\n",
      "Total params: 124,652\n",
      "Trainable params: 124,652\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Masking(mask_value = -1., input_shape=(None,1)))\n",
    "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.RNN(tfa.rnn.LayerNormLSTMCell(60, recurrent_dropout=0.25), return_sequences=True)))\n",
    "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.RNN(tfa.rnn.LayerNormLSTMCell(60, recurrent_dropout=0.25), return_sequences=False)))\n",
    "model.add(tf.keras.layers.Dropout(0.3))\n",
    "model.add(tf.keras.layers.Dense(30, activation='elu', kernel_initializer='he_normal'))\n",
    "model.add(tf.keras.layers.Dropout(0.3))\n",
    "model.add(tf.keras.layers.Dense(2))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    if epoch < 4:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.1)\n",
    "\n",
    "scheduler_cb = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "earlystopping_cb = tf.keras.callbacks.EarlyStopping(patience=6, restore_best_weights=True)\n",
    "\n",
    "checkpoints = \"checkpoints/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "checkpoint_filepath = checkpoints + \"checkpoint_layernorm_{epoch:02d}\"\n",
    "\n",
    "model_checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=False,\n",
    "    save_freq='epoch')\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(0.01)\n",
    "model.compile(loss=\"mse\", optimizer=optimizer, metrics=[tf.keras.metrics.MeanAbsoluteError()])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "586/586 [==============================] - 434s 653ms/step - loss: 0.0525 - mean_absolute_error: 0.1579 - val_loss: 0.0203 - val_mean_absolute_error: 0.0866\n",
      "Epoch 2/50\n",
      "586/586 [==============================] - 405s 619ms/step - loss: 0.0218 - mean_absolute_error: 0.0996 - val_loss: 0.0196 - val_mean_absolute_error: 0.0797\n",
      "Epoch 3/50\n",
      "586/586 [==============================] - 395s 604ms/step - loss: 0.0210 - mean_absolute_error: 0.0955 - val_loss: 0.0194 - val_mean_absolute_error: 0.0763\n",
      "Epoch 4/50\n",
      "586/586 [==============================] - 396s 606ms/step - loss: 0.0206 - mean_absolute_error: 0.0937 - val_loss: 0.0194 - val_mean_absolute_error: 0.0767\n",
      "Epoch 5/50\n",
      "586/586 [==============================] - 393s 602ms/step - loss: 0.0204 - mean_absolute_error: 0.0926 - val_loss: 0.0199 - val_mean_absolute_error: 0.0794\n",
      "Epoch 6/50\n",
      "586/586 [==============================] - 394s 604ms/step - loss: 0.0204 - mean_absolute_error: 0.0924 - val_loss: 0.0198 - val_mean_absolute_error: 0.0786\n",
      "Epoch 7/50\n",
      "586/586 [==============================] - 395s 605ms/step - loss: 0.0201 - mean_absolute_error: 0.0916 - val_loss: 0.0202 - val_mean_absolute_error: 0.0791\n",
      "Epoch 8/50\n",
      "586/586 [==============================] - 395s 605ms/step - loss: 0.0200 - mean_absolute_error: 0.0913 - val_loss: 0.0197 - val_mean_absolute_error: 0.0820\n",
      "Epoch 9/50\n",
      "586/586 [==============================] - 394s 604ms/step - loss: 0.0200 - mean_absolute_error: 0.0909 - val_loss: 0.0191 - val_mean_absolute_error: 0.0752\n",
      "Epoch 10/50\n",
      "586/586 [==============================] - 395s 605ms/step - loss: 0.0200 - mean_absolute_error: 0.0906 - val_loss: 0.0194 - val_mean_absolute_error: 0.0775\n",
      "Epoch 11/50\n",
      "586/586 [==============================] - 394s 603ms/step - loss: 0.0199 - mean_absolute_error: 0.0902 - val_loss: 0.0199 - val_mean_absolute_error: 0.0797\n",
      "Epoch 12/50\n",
      "586/586 [==============================] - 393s 603ms/step - loss: 0.0198 - mean_absolute_error: 0.0898 - val_loss: 0.0199 - val_mean_absolute_error: 0.0788\n",
      "Epoch 13/50\n",
      "586/586 [==============================] - 393s 602ms/step - loss: 0.0198 - mean_absolute_error: 0.0895 - val_loss: 0.0193 - val_mean_absolute_error: 0.0752\n",
      "Epoch 14/50\n",
      "586/586 [==============================] - 393s 603ms/step - loss: 0.0196 - mean_absolute_error: 0.0889 - val_loss: 0.0193 - val_mean_absolute_error: 0.0732\n",
      "Epoch 15/50\n",
      "586/586 [==============================] - 393s 602ms/step - loss: 0.0195 - mean_absolute_error: 0.0886 - val_loss: 0.0194 - val_mean_absolute_error: 0.0746\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_dataset, epochs=50, validation_data=validate_dataset, \n",
    "                    callbacks=[model_checkpoint_cb, scheduler_cb, earlystopping_cb])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fd6327890f0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model = tf.keras.models.load_model(\"checkpoints/20210609-161957checkpoint_lr01_06\")\n",
    "#model.load_weights(\"checkpoints/20210609-171355checkpoint_lr01_05\")\n",
    "model.load_weights(\"checkpoints/20210610-174049checkpoint_layernorm_05\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup test dataset\n",
    "batch_size = 1024\n",
    "test_filepath_dataset = tf.data.TFRecordDataset.list_files(\"data/test/*.tfrecord\", shuffle=True)\n",
    "\n",
    "test_num_files = test_filepath_dataset.cardinality()\n",
    "\n",
    "test_dataset = test_filepath_dataset.interleave(\n",
    "    lambda x: tf.data.TFRecordDataset(x),\n",
    "    cycle_length=test_num_files,\n",
    "    block_length=1,\n",
    "    num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",
    "    deterministic=False)\n",
    "\n",
    "def test_preprocess_and_split_into_tuples(tfrecord):\n",
    "    # parse sequence example and normalise\n",
    "    example = tf.io.parse_sequence_example(tfrecord, context_features=context_features, sequence_features=sequence_features)\n",
    "    trait_frequencies = example[1][\"raw_trait_frequencies\"]\n",
    "    pop_size_norm = (tf.cast(example[0][\"population_size\"], dtype=tf.float32) - pop_size_min) / (pop_size_max - pop_size_min)\n",
    "    sc_norm = (tf.cast(example[0][\"selection_coefficient\"], dtype=tf.float32) - sc_min) / (sc_max - sc_min)\n",
    "    # filter dataset to only include trials in which the trait survives at least min_survival\n",
    "    trait_frequencies = tf.gather(trait_frequencies, tf.where(trait_frequencies.row_lengths() > 12), axis=0)\n",
    "    # note that, unlike in the train/valid cases where we randomly sample num_trait_data trajectories,\n",
    "    # we analyse all trajectories (> min_survival) for the test set        \n",
    "    trait_frequencies = trait_frequencies[:, :, 0:max_time_steps] \n",
    "    trait_frequencies = tf.squeeze(trait_frequencies, axis=1)\n",
    "    n_trajectories = trait_frequencies.nrows()\n",
    "    # cast labels into a list of tensors matching the features\n",
    "    label_tensors = tf.reshape(tf.stack((tf.repeat(pop_size_norm, n_trajectories), \n",
    "                                         tf.repeat(sc_norm, n_trajectories)), axis=1), shape=(n_trajectories, 2))   \n",
    "    # convert trait_frequencies to dense tensor (with mask value of -1)\n",
    "    trait_frequencies = trait_frequencies.to_tensor(default_value = -1., \n",
    "                                                    shape=(n_trajectories, max_time_steps))\n",
    "    \n",
    "    # add final dimension (keras requires an input shape of (batch_size, timesteps, features))\n",
    "    trait_frequencies = tf.expand_dims(trait_frequencies, axis=2)\n",
    "\n",
    "    return tf.data.Dataset.zip((tf.data.Dataset.from_tensor_slices(trait_frequencies), tf.data.Dataset.from_tensor_slices(label_tensors)))\n",
    "\n",
    "test_dataset = test_dataset.interleave(\n",
    "    test_preprocess_and_split_into_tuples,\n",
    "    cycle_length=test_num_files,\n",
    "    block_length=1,\n",
    "    num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",
    "    deterministic=False)\n",
    "\n",
    "# no need to shuffle test set\n",
    "test_dataset = test_dataset.shuffle(10000).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 53s 166ms/step - loss: 0.0030 - mean_absolute_error: 0.0344\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0030169931706041098, 0.03436065465211868]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_dataset.take(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('best_model_june10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
